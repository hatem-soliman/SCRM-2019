expectation maximization classification in python
==================================================================================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
from matplotlib import style
style.use('fivethirtyeight')
from sklearn.datasets.samples_generator import make_blobs
import numpy as np
from scipy.stats import multivariate_normal
gmm = GaussianMixture(n_components = 2, tol=0.000001)
gmm.fit(np.expand_dims(data, 1))
Gaussian_nr = 1
print('Input Normal_distb {:}: μ = {:.2}, σ = {:.2}'.format("1", Mean1, Standard_dev1))
print('Input Normal_distb {:}: μ = {:.2}, σ = {:.2}'.format("2", Mean2, Standard_dev2))


for mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):
    print('Normal_distb {:}: μ = {:.2}, σ = {:.2}, weight = {:.2}'.format(Gaussian_nr, mu, sd, p))
    g_s = stats.norm(mu, sd).pdf(x) * p
    plt.plot(x, g_s, label='gaussian sklearn');
    Gaussian_nr += 1

sns.distplot(data, bins=20, kde=False, norm_hist=True)
gmm_sum = np.exp([gmm.score_samples(e.reshape(-1, 1)) for e in x]) 
plt.plot(x, gmm_sum, label='gaussian mixture');
plt.legend();


X,Y = make_blobs(cluster_std=1.5,random_state=20,n_samples=500,centers=3)

# Stratch dataset to get ellipsoid data
X = np.dot(X,np.random.RandomState(0).randn(2,2))


class GMM:

    def __init__(self,X,number_of_sources,iterations):
        self.iterations = iterations
        self.number_of_sources = number_of_sources
        self.X = X
        self.mu = None
        self.pi = None
        self.cov = None
        self.XY = None
        
    

    """Define a function which runs for iterations, iterations"""
    def run(self):
        self.reg_cov = 1e-6*np.identity(len(self.X[0]))
        x,y = np.meshgrid(np.sort(self.X[:,0]),np.sort(self.X[:,1]))
        self.XY = np.array([x.flatten(),y.flatten()]).T
           
                    
        """ 1. Set the initial mu, covariance and pi values"""
        self.mu = np.random.randint(min(self.X[:,0]),max(self.X[:,0]),size=(self.number_of_sources,len(self.X[0]))) # This is a nxm matrix since we assume n sources (n Gaussians) where each has m dimensions
        self.cov = np.zeros((self.number_of_sources,len(X[0]),len(X[0]))) # We need a nxmxm covariance matrix for each source since we have m features --> We create symmetric covariance matrices with ones on the digonal
        for dim in range(len(self.cov)):
            np.fill_diagonal(self.cov[dim],5)


        self.pi = np.ones(self.number_of_sources)/self.number_of_sources # Are "Fractions"
        log_likelihoods = [] # In this list we store the log likehoods per iteration and plot them in the end to check if
                             # if we have converged
            
        """Plot the initial state"""    
        fig = plt.figure(figsize=(10,10))
        ax0 = fig.add_subplot(111)
        ax0.scatter(self.X[:,0],self.X[:,1])
        ax0.set_title('Initial state')
        for m,c in zip(self.mu,self.cov):
            c += self.reg_cov
            multi_normal = multivariate_normal(mean=m,cov=c)
            ax0.contour(np.sort(self.X[:,0]),np.sort(self.X[:,1]),multi_normal.pdf(self.XY).reshape(len(self.X),len(self.X)),colors='black',alpha=0.3)
            ax0.scatter(m[0],m[1],c='grey',zorder=10,s=100)
        
        for i in range(self.iterations):               

            """E Step"""
            r_ic = np.zeros((len(self.X),len(self.cov)))

            for m,co,p,r in zip(self.mu,self.cov,self.pi,range(len(r_ic[0]))):
                co+=self.reg_cov
                mn = multivariate_normal(mean=m,cov=co)
                r_ic[:,r] = p*mn.pdf(self.X)/np.sum([pi_c*multivariate_normal(mean=mu_c,cov=cov_c).pdf(X) for pi_c,mu_c,cov_c in zip(self.pi,self.mu,self.cov+self.reg_cov)],axis=0)

          
        

            # Calculate the new mean vector and new covariance matrices, based on the probable membership of the single x_i to classes c --> r_ic
            self.mu = []
            self.cov = []
            self.pi = []
            log_likelihood = []

            for c in range(len(r_ic[0])):
                m_c = np.sum(r_ic[:,c],axis=0)
                mu_c = (1/m_c)*np.sum(self.X*r_ic[:,c].reshape(len(self.X),1),axis=0)
                self.mu.append(mu_c)

                # Calculate the covariance matrix per source based on the new mean
                self.cov.append(((1/m_c)*np.dot((np.array(r_ic[:,c]).reshape(len(self.X),1)*(self.X-mu_c)).T,(self.X-mu_c)))+self.reg_cov)
                # Calculate pi_new which is the "fraction of points" respectively the fraction of the probability assigned to each source 
                self.pi.append(m_c/np.sum(r_ic)) 

            
            
            """Log likelihood"""
            log_likelihoods.append(np.log(np.sum([k*multivariate_normal(self.mu[i],self.cov[j]).pdf(X) for k,i,j in zip(self.pi,range(len(self.mu)),range(len(self.cov)))])))

            

            """
            This process of E step followed by a M step is now iterated a number of n times. In the second step for instance,
            we use the calculated pi_new, mu_new and cov_new to calculate the new r_ic which are then used in the second M step
            to calculat the mu_new2 and cov_new2 and so on....
            """

        fig2 = plt.figure(figsize=(10,10))
        ax1 = fig2.add_subplot(111) 
        ax1.set_title('Log-Likelihood')
        ax1.plot(range(0,self.iterations,1),log_likelihoods)
        #plt.show()
    
    """Predict the membership of an unseen, new datapoint"""
    def predict(self,Y):
        # PLot the point onto the fittet gaussians
        fig3 = plt.figure(figsize=(10,10))
        ax2 = fig3.add_subplot(111)
        ax2.scatter(self.X[:,0],self.X[:,1])
        for m,c in zip(self.mu,self.cov):
            multi_normal = multivariate_normal(mean=m,cov=c)
            ax2.contour(np.sort(self.X[:,0]),np.sort(self.X[:,1]),multi_normal.pdf(self.XY).reshape(len(self.X),len(self.X)),colors='black',alpha=0.3)
            ax2.scatter(m[0],m[1],c='grey',zorder=10,s=100)
            ax2.set_title('Final state')
            for y in Y:
                ax2.scatter(y[0],y[1],c='orange',zorder=10,s=100)
        prediction = []        
        for m,c in zip(self.mu,self.cov):  
            #print(c)
            prediction.append(multivariate_normal(mean=m,cov=c).pdf(Y)/np.sum([multivariate_normal(mean=mean,cov=cov).pdf(Y) for mean,cov in zip(self.mu,self.cov)]))
        #plt.show()
        return prediction
         
    
    
GMM = GMM(X,3,50)     
GMM.run()
GMM.predict([[0.5,0.5]])
==============================================================================================
SLCN is based on the local classifier per node (LCN)
==============================================================================================
"""
Hierarchical classifier interface.

"""
import numpy as np
from networkx import DiGraph, is_tree
from scipy.sparse import csr_matrix
from sklearn.base import (
    BaseEstimator,
    ClassifierMixin,
    MetaEstimatorMixin,
    clone,
)
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.utils.multiclass import check_classification_targets
from sklearn.utils.validation import (
    check_array,
    check_consistent_length,
    check_is_fitted,
    check_X_y,
)

from sklearn_hierarchical_classification.array import (
    apply_along_rows,
    apply_rollup_Xy,
    apply_rollup_Xy_raw,
    extract_rows_csr,
    flatten_list,
    nnz_rows_ix,
)
from sklearn_hierarchical_classification.constants import (
    CLASSIFIER,
    DEFAULT,
    METAFEATURES,
    ROOT,
)
from sklearn_hierarchical_classification.decorators import logger
from sklearn_hierarchical_classification.dummy import DummyProgress
from sklearn_hierarchical_classification.graph import make_flat_hierarchy, rollup_nodes
from sklearn_hierarchical_classification.validation import is_estimator, validate_parameters


@logger
class HierarchicalClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):

    def __init__(
        self,
        base_estimator=None,
        class_hierarchy=None,
        prediction_depth="mlnp",
        algorithm="lcpn",
        training_strategy=None,
        stopping_criteria=None,
        root=ROOT,
        progress_wrapper=None,
        feature_extraction="preprocessed",
        mlb=None,
        mlb_prediction_threshold=0.,
        use_decision_function=False,
    ):
        self.base_estimator = base_estimator
        self.class_hierarchy = class_hierarchy
        self.prediction_depth = prediction_depth
        self.algorithm = algorithm
        self.training_strategy = training_strategy
        self.stopping_criteria = stopping_criteria
        self.root = root
        self.progress_wrapper = progress_wrapper
        self.feature_extraction = feature_extraction
        self.mlb = mlb
        self.mlb_prediction_threshold = mlb_prediction_threshold
        self.use_decision_function = use_decision_function

    def fit(self, X, y=None, sample_weight=None):
        """Fit underlying classifiers.

        Parameters
        ----------
        X : (sparse) array-like, shape = [n_samples, n_features]
            Data.

        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
            Multi-class targets. An indicator matrix turns on multilabel
            classification.

        sample_weight : array-like, shape (n_samples,), optional (default=None)
            Weights applied to individual samples (1. for unweighted).

        Returns
        -------
        self

        """
        if self.feature_extraction == "raw":
            # In raw mode, only validate targets (y) format and
            # that targets and training data (X) are of same cardinality, since
            # X will in general not be a 2D feature matrix, but rather the raw training examples,
            # e.g. text snippets or images.
            y = check_array(
                y,
                accept_sparse="csr",
                force_all_finite=True,
                ensure_2d=False,
                dtype=None,
            )
            if len(X) != y.shape[0]:
                raise ValueError("bad input shape: len(X) != y.shape[0]")
        else:
            X, y = check_X_y(X, y, accept_sparse="csr")

        check_classification_targets(y)
        if sample_weight is not None:
            check_consistent_length(y, sample_weight)

        # Check that parameter assignment is consistent
        self._check_parameters()

        # Initialize NetworkX Graph from input class hierarchy
        self.class_hierarchy_ = self.class_hierarchy or make_flat_hierarchy(list(np.unique(y)), root=self.root)
        self.graph_ = DiGraph(self.class_hierarchy_)
        self.is_tree_ = is_tree(self.graph_)
        self.classes_ = list(
            node
            for node in self.graph_.nodes()
            if node != self.root
        )

        if self.feature_extraction == "preprocessed":
            # When not in raw mode, recursively build training feature sets for each node in graph
            with self._progress(total=self.n_classes_ + 1, desc="Building features") as progress:
                self._recursive_build_features(X, y, node_id=self.root, progress=progress)

        # Recursively train base classifiers
        with self._progress(total=self.n_classes_ + 1, desc="Training base classifiers") as progress:
            self._recursive_train_local_classifiers(X, y, node_id=self.root, progress=progress)

        return self

    def predict(self, X):
        """Predict multi-class targets using underlying estimators.

        Parameters
        ----------
        X : (sparse) array-like, shape = [n_samples, n_features]
            Data.

        Returns
        -------
        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes].
            Predicted multi-class targets.

        """
        check_is_fitted(self, "graph_")

        def _classify(x):
            path, _ = self._recursive_predict(x, root=self.root)
            if self.mlb:
                return path
            else:
                return path[-1]

        if self.feature_extraction == "raw":
            return np.array([
                _classify(X[i])
                for i in range(len(X))
            ])
        else:
            X = check_array(X, accept_sparse="csr")

        y_pred = apply_along_rows(_classify, X=X)
        return y_pred

    def predict_proba(self, X):
        """
        Return probability estimates for the test vector X.

        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]

        Returns
        -------
        C : array-like, shape = [n_samples, n_classes]
            Returns the probability of the samples for each class in
            the model. The columns correspond to the classes in sorted
            order, as they appear in the attribute `classes_`.
        """
        check_is_fitted(self, "graph_")

        def _classify(x):
            _, scores = self._recursive_predict(x, root=self.root)
            return scores

        if self.feature_extraction == "raw":
            return np.array([
                _classify(X[i])
                for i in range(len(X))
            ])
        else:
            X = check_array(X, accept_sparse="csr")

        y_pred = apply_along_rows(_classify, X=X)
        return y_pred

    @property
    def n_classes_(self):
        return len(self.classes_)

    def _check_parameters(self):
        """Check the parameter assignment is valid and internally consistent."""
        validate_parameters(self)

    def _recursive_build_features(self, X, y, node_id, progress):
      
        if "X" in self.graph_.nodes[node_id]:
            # Already visited this node in feature building phase
            return self.graph_.nodes[node_id]["X"]

        self.logger.debug("Building features for node: %s", node_id)
        progress.update(1)

        if self.graph_.out_degree(node_id) == 0:
            # Leaf node
            indices = np.flatnonzero(y == node_id)
            self.graph_.nodes[node_id]["X"] = self._build_features(
                X=X,
                y=y,
                indices=indices,
            )

            return self.graph_.nodes[node_id]["X"]

        # Non-leaf node
        if self.feature_extraction == "raw":
            self.graph_.nodes[node_id]["X"] = []
        else:
            self.graph_.nodes[node_id]["X"] = csr_matrix(
                X.shape,
                dtype=X.dtype,
            )

        for child_node_id in self.graph_.successors(node_id):
            self.graph_.nodes[node_id]["X"] += \
                self._recursive_build_features(
                    X=X,
                    y=y,
                    node_id=child_node_id,
                    progress=progress,
                )

        # Build and store metafeatures for node
        self.graph_.nodes[node_id][METAFEATURES] = self._build_metafeatures(
            X=self.graph_.nodes[node_id]["X"],
            y=y,
        )

        # Append training data tagged with current (intermediate) node if any, and propagate up
        if not np.issubdtype(type(node_id), y.dtype):
            # If current intermediate node id type is different than that of targets array, dont bother.
            # Nb. doing this check explicitly to avoid FutureWarning, see:
            # https://stackoverflow.com/questions/40659212/futurewarning-elementwise-comparison-failed-returning-scalar-but-in-the-futur
            return self.graph_.nodes[node_id]["X"]

        indices = np.flatnonzero(y == node_id)
        X_out = self.graph_.nodes[node_id]["X"] + self._build_features(
            X=X,
            y=y,
            indices=indices,
        )
        return X_out

    def _build_features(self, X, y, indices):
        if self.feature_extraction == "raw":
            X_ = [X[ix] for ix in indices]
        else:
            X_ = extract_rows_csr(X, indices)

        # Perform feature selection
        X_ = self._select_features(X=X_, y=np.array(y)[indices])

        return X_

    def _select_features(self, X, y):
        """
        Perform feature selection for training data.

        Can be overridden by a sub-class to implement feature selection logic.

        """
        return X

    def _build_metafeatures(self, X, y):
      
        ----------
        X : (sparse) array-like, shape = [n_samples, n_features]
            The training data matrix at current node.

        Returns
        -------
        metafeatures : dict
            Python dictionary of meta-features. The following meta-features are computed by default:
            * "n_samples" - Number of samples used to train classifier at given node.
            * "n_targets" - Number of targets (classes) to classify into at given node.

        """
        if self.feature_extraction == "raw":
            # In raw mode, we do not know which training examples are "zeroed out" for which node
            # since we do not recursively build features until the recursive training phase which comes afterwards.
            # Therefore, the number of targets is simply the number of unique labels in y
            return dict(
                n_samples=len(X),
                n_targets=len(np.unique(y)),
            )

        # Indices of non-zero rows in X, i.e rows corresponding to relevant samples for this node.
        ix = nnz_rows_ix(X)

        return dict(
            n_samples=len(ix),
            n_targets=len(np.unique(y[ix])),
        )

    def _recursive_train_local_classifiers(self, X, y, node_id, progress):
        if CLASSIFIER in self.graph_.nodes[node_id]:
            # Already trained classifier at this node, skip
            return

        progress.update(1)
        self._train_local_classifier(X, y, node_id)

        for child_node_id in self.graph_.successors(node_id):
            self._recursive_train_local_classifiers(
                X=X,
                y=y,
                node_id=child_node_id,
                progress=progress,
            )

    def _train_local_classifier(self, X, y, node_id):
        if self.graph_.out_degree(node_id) == 0:
            # Leaf node
            if self.algorithm == "lcpn":
                # Leaf nodes do not get a classifier assigned in LCPN algorithm mode.
                self.logger.debug(
                    "_train_local_classifier() - skipping leaf node %s when algorithm is 'lcpn'",
                    node_id,
                )
                return

        if self.feature_extraction == "raw":
            X_ = X
            nnz_rows = range(len(X))
            Xl = len(X_)
        else:
            X = self.graph_.nodes[node_id]["X"]
            nnz_rows = nnz_rows_ix(X)
            X_ = X[nnz_rows, :]
            Xl = X_.shape

        y_rolled_up = rollup_nodes(
            graph=self.graph_,
            source=node_id,
            targets=[y[idx] for idx in nnz_rows],
            mlb=self.mlb
        )

        if self.is_tree_:
            if self.mlb is None:
                y_ = flatten_list(y_rolled_up)
            else:
                y_ = self.mlb.transform(y_rolled_up)
                # take all non zero, only compare in side the siblings
                idx = np.where(y_.sum(1) > 0)[0]
                y_ = y_[idx, :]
                if self.feature_extraction == "raw":
                    X_ = [X_[tk] for tk in idx]
                else:
                    X_ = X_[idx, :]
        else:
            # Class hierarchy graph is a DAG
            if self.feature_extraction == "raw":
                X_, y_ = apply_rollup_Xy_raw(X_, y_rolled_up)
            else:
                X_, y_ = apply_rollup_Xy(X_, y_rolled_up)

        num_targets = len(np.unique(y_))

        self.logger.debug(
            "_train_local_classifier() - Training local classifier for node: %s, X_.shape: %s, len(y): %s, n_targets: %s",  # noqa:E501
            node_id,
            Xl,
            len(y_),
            num_targets,
        )

        if self.feature_extraction == "preprocessed" and X_.shape[0] == 0:
            # No training data could be materialized for current node
            # TODO: support a "strict" mode flag to explicitly enable/disable fallback logic here?
            self.logger.warning(
                "_train_local_classifier() - not enough training data available to train, classification in branch will terminate at node %s",  # noqa:E501
                node_id,
            )
            return
        elif num_targets == 1:
            # Training data could be materialized for only a single target at current node
            # TODO: support a "strict" mode flag to explicitly enable/disable fallback logic here?
            constant = y_[0]
            self.logger.debug(
                "_train_local_classifier() - only a single target (child node) available to train classifier for node %s, Will trivially predict %s",  # noqa:E501
                node_id,
                constant,
            )

            clf = DummyClassifier(strategy="constant", constant=constant)
        else:
            clf = self._base_estimator_for(node_id)

        if self.feature_extraction == "raw":
            if len(X_) > 0:
                clf.fit(X=X_, y=y_)
                self.logger.debug(
                    "_train_local_classifier() - training node %s ",  # noqa:E501
                    node_id,
                )
                self.graph_.nodes[node_id][CLASSIFIER] = clf
            else:
                self.logger.debug(
                    "_train_local_classifier() - could not train  node %s ",  # noqa:E501
                    node_id,
                )

        else:
            clf.fit(X=X_, y=y_)
            self.graph_.nodes[node_id][CLASSIFIER] = clf

    def _recursive_predict(self, x, root):  # noqa:C901 TODO: refactor
        if CLASSIFIER not in self.graph_.nodes[root]:
            return None, None

        clf = self.graph_.nodes[root][CLASSIFIER]
        path = [root]
        path_proba = []
        class_proba = np.zeros_like(self.classes_, dtype=np.float64)

        while clf:
            if self.use_decision_function and hasattr(clf, "decision_function"):
                if self.feature_extraction == "raw":
                    probs = clf.decision_function([x])
                    argmax = np.argmax(probs)
                    score = probs[0, argmax]

                else:
                    probs = clf.decision_function(x)
                    argmax = np.argmax(probs)
                    score = probs[argmax]
            else:
                probs = clf.predict_proba(x)[0]
                argmax = np.argmax(probs)
                score = probs[argmax]

            path_proba.append(score)
            if self.mlb is not None:
                predictions = []

            # Report probabilities in terms of complete class hierarchy
            if len(clf.classes_) == 1:
                prediction = clf.classes_[0]

            for local_class_idx, class_ in enumerate(clf.classes_):
                if self.mlb:
                    # when we have a multi-label binarizer
                    class_idx = class_
                    class_proba[class_idx] = probs[0, local_class_idx]
                    if class_proba[class_idx] > self.mlb_prediction_threshold:
                        predictions.append(self.mlb.classes_[class_])
                else:
                    try:
                        class_idx = self.classes_.index(class_)
                    except ValueError:
                        # This may happen if the classes_ enumeration we construct during fit()
                        # has a mismatch with the individual node classifiers" classes_.
                        self.logger.error(
                            "Could not find index in self.classes_ for class_ = '%s' (type: %s). path: %s",
                            class_,
                            type(class_),
                            path,
                        )
                        raise
                    if len(probs.shape) > 1 and probs.shape[0] == 1:
                        class_proba[class_idx] = probs[0, local_class_idx]
                        if local_class_idx == argmax:
                            prediction = class_
                    else:
                        class_proba[class_idx] = probs[local_class_idx]
                        if local_class_idx == argmax:
                            prediction = class_

            if self.mlb is None:
                if self._should_early_terminate(
                    current_node=path[-1],
                    prediction=prediction,
                    score=score,
                ):
                    break

                # Update current path
                path.append(prediction)
                clf = self.graph_.nodes[prediction].get(CLASSIFIER, None)
            else:
                clf = None
                for prediction in predictions:
                    pred_path, preds_prob = self._recursive_predict(x, prediction)
                    path.append(prediction)
                    if preds_prob is not None:
                        class_proba += preds_prob
                        path.extend(pred_path)

        return path, class_proba

    def _should_early_terminate(self, current_node, prediction, score):
        """
        Evaluate whether classification should terminate at given step.

        This depends on whether early-termination, as dictated by the the "prediction_depth"
          and "stopping_criteria" parameters, is triggered.

        """
        if self.prediction_depth != "nmlnp":
            # Prediction depth parameter does not allow for early termination
            return False

        if (
            isinstance(self.stopping_criteria, float)
            and score < self.stopping_criteria
        ):
            if current_node == self.root:
                return False

            self.logger.debug(
                "_should_early_terminate() - score %s < %s, terminating at node %s",
                score,
                self.stopping_criteria,
                current_node,
            )
            return True
        elif callable(self.stopping_criteria):
            return self.stopping_criteria(
                current_node=self.graph_.nodes[current_node],
                prediction=prediction,
                score=score,
            )

        return False

    def _base_estimator_for(self, node_id):
        base_estimator = None
        if self.base_estimator is None:
            # No base estimator specified by user, try to pick best one
            base_estimator = self._make_base_estimator(node_id)

        elif isinstance(self.base_estimator, dict):
            # User provided dictionary mapping nodes to estimators
            if node_id in self.base_estimator:
                base_estimator = self.base_estimator[node_id]
            else:
                base_estimator = self.base_estimator[DEFAULT]

        elif is_estimator(self.base_estimator):
            # Single base estimator object, return a copy
            base_estimator = self.base_estimator

        else:
            # By default, treat as callable factory
            base_estimator = self.base_estimator(node_id=node_id, graph=self.graph_)

        return clone(base_estimator)

    def _make_base_estimator(self, node_id):
        """Create a default base estimator if a more specific one was not chosen by user."""
        return LogisticRegression(
            solver="lbfgs",
            max_iter=1000,
            multi_class="multinomial",
        )

    def _progress(self, total, desc, **kwargs):
        if self.progress_wrapper:
            return self.progress_wrapper(total=total, desc=desc)
        else:
            return DummyProgress()
======================================================================================